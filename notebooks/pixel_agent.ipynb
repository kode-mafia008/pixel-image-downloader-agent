{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b594f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.schema import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# State definition for the agent\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    url: str\n",
    "    images: List[Dict[str, str]]\n",
    "    download_path: str\n",
    "    status: str\n",
    "    errors: List[str]\n",
    "\n",
    "@dataclass\n",
    "class ImageInfo:\n",
    "    url: str\n",
    "    filename: str\n",
    "    alt_text: str = \"\"\n",
    "    photographer: str = \"\"\n",
    "\n",
    "class PexelsScraperTool(BaseTool):\n",
    "    name = \"pexels_scraper\"\n",
    "    description = \"Scrapes Pexels search page to extract image URLs and metadata\"\n",
    "\n",
    "    def _run(self, url: str) -> Dict[str, Any]:\n",
    "        \"\"\"Scrape Pexels page and extract image information\"\"\"\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            images = []\n",
    "            \n",
    "            # Find image containers - Pexels uses specific CSS classes\n",
    "            image_containers = soup.find_all('article', class_=re.compile(r'.*photo.*'))\n",
    "            \n",
    "            if not image_containers:\n",
    "                # Alternative approach - look for img tags with src containing pexels\n",
    "                img_tags = soup.find_all('img', src=re.compile(r'.*pexels.*'))\n",
    "                for img in img_tags:\n",
    "                    if img.get('src'):\n",
    "                        images.append({\n",
    "                            'url': img['src'],\n",
    "                            'filename': self._generate_filename(img['src']),\n",
    "                            'alt_text': img.get('alt', ''),\n",
    "                            'photographer': ''\n",
    "                        })\n",
    "            else:\n",
    "                for container in image_containers:\n",
    "                    img_tag = container.find('img')\n",
    "                    if img_tag and img_tag.get('src'):\n",
    "                        # Extract photographer info if available\n",
    "                        photographer_elem = container.find('a', href=re.compile(r'/@.*'))\n",
    "                        photographer = photographer_elem.text.strip() if photographer_elem else \"\"\n",
    "                        \n",
    "                        images.append({\n",
    "                            'url': img_tag['src'],\n",
    "                            'filename': self._generate_filename(img_tag['src']),\n",
    "                            'alt_text': img_tag.get('alt', ''),\n",
    "                            'photographer': photographer\n",
    "                        })\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'images': images,\n",
    "                'count': len(images),\n",
    "                'message': f\"Found {len(images)} images\"\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'images': [],\n",
    "                'count': 0,\n",
    "                'message': f\"Error scraping page: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    def _generate_filename(self, url: str) -> str:\n",
    "        \"\"\"Generate a filename from the image URL\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        filename = os.path.basename(parsed.path)\n",
    "        if not filename or not filename.endswith(('.jpg', '.jpeg', '.png', '.webp')):\n",
    "            # Extract ID from Pexels URL pattern\n",
    "            match = re.search(r'/(\\d+)/', url)\n",
    "            if match:\n",
    "                filename = f\"pexels_{match.group(1)}.jpg\"\n",
    "            else:\n",
    "                filename = f\"pexels_image_{int(time.time())}.jpg\"\n",
    "        return filename\n",
    "\n",
    "class ImageDownloaderTool(BaseTool):\n",
    "    name = \"image_downloader\"\n",
    "    description = \"Downloads images from URLs to local directory\"\n",
    "\n",
    "    def _run(self, images: List[Dict], download_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Download images to specified directory\"\"\"\n",
    "        try:\n",
    "            # Create download directory if it doesn't exist\n",
    "            Path(download_path).mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            downloaded = []\n",
    "            failed = []\n",
    "            \n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            }\n",
    "            \n",
    "            for img_info in images:\n",
    "                try:\n",
    "                    # Clean up the URL - remove query parameters for better quality\n",
    "                    clean_url = img_info['url'].split('?')[0]\n",
    "                    \n",
    "                    response = requests.get(clean_url, headers=headers, timeout=30)\n",
    "                    response.raise_for_status()\n",
    "                    \n",
    "                    file_path = os.path.join(download_path, img_info['filename'])\n",
    "                    \n",
    "                    # Avoid overwriting existing files\n",
    "                    counter = 1\n",
    "                    original_path = file_path\n",
    "                    while os.path.exists(file_path):\n",
    "                        name, ext = os.path.splitext(original_path)\n",
    "                        file_path = f\"{name}_{counter}{ext}\"\n",
    "                        counter += 1\n",
    "                    \n",
    "                    with open(file_path, 'wb') as f:\n",
    "                        f.write(response.content)\n",
    "                    \n",
    "                    downloaded.append({\n",
    "                        'filename': os.path.basename(file_path),\n",
    "                        'path': file_path,\n",
    "                        'photographer': img_info.get('photographer', ''),\n",
    "                        'alt_text': img_info.get('alt_text', '')\n",
    "                    })\n",
    "                    \n",
    "                    # Small delay to be respectful to the server\n",
    "                    time.sleep(0.5)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    failed.append({\n",
    "                        'url': img_info['url'],\n",
    "                        'filename': img_info['filename'],\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "                    continue\n",
    "            \n",
    "            # Save metadata\n",
    "            metadata_file = os.path.join(download_path, 'download_metadata.json')\n",
    "            metadata = {\n",
    "                'downloaded': downloaded,\n",
    "                'failed': failed,\n",
    "                'total_downloaded': len(downloaded),\n",
    "                'total_failed': len(failed),\n",
    "                'download_date': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "            \n",
    "            with open(metadata_file, 'w') as f:\n",
    "                json.dump(metadata, f, indent=2)\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'downloaded': len(downloaded),\n",
    "                'failed': len(failed),\n",
    "                'metadata_file': metadata_file,\n",
    "                'message': f\"Downloaded {len(downloaded)} images, {len(failed)} failed\"\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'downloaded': 0,\n",
    "                'failed': len(images),\n",
    "                'message': f\"Error downloading images: {str(e)}\"\n",
    "            }\n",
    "\n",
    "class PexelsDownloaderAgent:\n",
    "    def __init__(self, openai_api_key: str = None):\n",
    "        self.scraper_tool = PexelsScraperTool()\n",
    "        self.downloader_tool = ImageDownloaderTool()\n",
    "        \n",
    "        # Initialize LLM (optional - for more intelligent decision making)\n",
    "        if openai_api_key:\n",
    "            self.llm = ChatOpenAI(api_key=openai_api_key, model=\"gpt-3.5-turbo\")\n",
    "        else:\n",
    "            self.llm = None\n",
    "        \n",
    "        # Build the graph\n",
    "        self.workflow = self._build_workflow()\n",
    "        self.app = self.workflow.compile()\n",
    "    \n",
    "    def _build_workflow(self) -> StateGraph:\n",
    "        \"\"\"Build the LangGraph workflow\"\"\"\n",
    "        workflow = StateGraph(AgentState)\n",
    "        \n",
    "        # Add nodes\n",
    "        workflow.add_node(\"validate_url\", self._validate_url)\n",
    "        workflow.add_node(\"scrape_images\", self._scrape_images)\n",
    "        workflow.add_node(\"download_images\", self._download_images)\n",
    "        workflow.add_node(\"finalize\", self._finalize)\n",
    "        \n",
    "        # Add edges\n",
    "        workflow.add_edge(\"validate_url\", \"scrape_images\")\n",
    "        workflow.add_edge(\"scrape_images\", \"download_images\")\n",
    "        workflow.add_edge(\"download_images\", \"finalize\")\n",
    "        workflow.add_edge(\"finalize\", END)\n",
    "        \n",
    "        # Set entry point\n",
    "        workflow.set_entry_point(\"validate_url\")\n",
    "        \n",
    "        return workflow\n",
    "    \n",
    "    def _validate_url(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Validate that the URL is a Pexels search URL\"\"\"\n",
    "        url = state[\"url\"]\n",
    "        \n",
    "        if not url.startswith(\"https://www.pexels.com\"):\n",
    "            state[\"status\"] = \"error\"\n",
    "            state[\"errors\"].append(\"URL must be from pexels.com\")\n",
    "            return state\n",
    "        \n",
    "        state[\"status\"] = \"validated\"\n",
    "        state[\"messages\"].append(AIMessage(content=f\"Validated URL: {url}\"))\n",
    "        return state\n",
    "    \n",
    "    def _scrape_images(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Scrape images from the Pexels page\"\"\"\n",
    "        if state[\"status\"] == \"error\":\n",
    "            return state\n",
    "        \n",
    "        result = self.scraper_tool._run(state[\"url\"])\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            state[\"images\"] = result[\"images\"]\n",
    "            state[\"status\"] = \"scraped\"\n",
    "            state[\"messages\"].append(AIMessage(\n",
    "                content=f\"Successfully scraped {result['count']} images from the page\"\n",
    "            ))\n",
    "        else:\n",
    "            state[\"status\"] = \"error\"\n",
    "            state[\"errors\"].append(result[\"message\"])\n",
    "            state[\"messages\"].append(AIMessage(\n",
    "                content=f\"Failed to scrape images: {result['message']}\"\n",
    "            ))\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _download_images(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Download the scraped images\"\"\"\n",
    "        if state[\"status\"] == \"error\" or not state[\"images\"]:\n",
    "            return state\n",
    "        \n",
    "        result = self.downloader_tool._run(state[\"images\"], state[\"download_path\"])\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            state[\"status\"] = \"downloaded\"\n",
    "            state[\"messages\"].append(AIMessage(\n",
    "                content=f\"Downloaded {result['downloaded']} images to {state['download_path']}\"\n",
    "            ))\n",
    "            if result[\"failed\"] > 0:\n",
    "                state[\"messages\"].append(AIMessage(\n",
    "                    content=f\"Warning: {result['failed']} images failed to download\"\n",
    "                ))\n",
    "        else:\n",
    "            state[\"status\"] = \"error\"\n",
    "            state[\"errors\"].append(result[\"message\"])\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _finalize(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Finalize the process and provide summary\"\"\"\n",
    "        if state[\"status\"] == \"downloaded\":\n",
    "            summary = f\"\"\"\n",
    "Download completed successfully!\n",
    "- URL: {state['url']}\n",
    "- Images found: {len(state['images'])}\n",
    "- Download location: {state['download_path']}\n",
    "- Status: {state['status']}\n",
    "\"\"\"\n",
    "        else:\n",
    "            summary = f\"\"\"\n",
    "Download failed:\n",
    "- URL: {state['url']}\n",
    "- Status: {state['status']}\n",
    "- Errors: {', '.join(state['errors'])}\n",
    "\"\"\"\n",
    "        \n",
    "        state[\"messages\"].append(AIMessage(content=summary))\n",
    "        return state\n",
    "    \n",
    "    def download_images(self, url: str, download_path: str = \"./downloads\") -> Dict[str, Any]:\n",
    "        \"\"\"Main method to download images from Pexels URL\"\"\"\n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=f\"Download images from: {url}\")],\n",
    "            \"url\": url,\n",
    "            \"images\": [],\n",
    "            \"download_path\": download_path,\n",
    "            \"status\": \"started\",\n",
    "            \"errors\": []\n",
    "        }\n",
    "        \n",
    "        # Run the workflow\n",
    "        final_state = self.app.invoke(initial_state)\n",
    "        \n",
    "        return {\n",
    "            \"status\": final_state[\"status\"],\n",
    "            \"images_count\": len(final_state[\"images\"]),\n",
    "            \"download_path\": final_state[\"download_path\"],\n",
    "            \"errors\": final_state[\"errors\"],\n",
    "            \"messages\": [msg.content for msg in final_state[\"messages\"]]\n",
    "        }\n",
    "\n",
    "# Usage example\n",
    "def main():\n",
    "    # Initialize the agent\n",
    "    agent = PexelsDownloaderAgent()\n",
    "    \n",
    "    # Example usage\n",
    "    pexels_url = \"https://www.pexels.com/search/astrophotography/\"\n",
    "    download_folder = \"./astrophotography_images\"\n",
    "    \n",
    "    print(\"Starting Pexels Image Download Agent...\")\n",
    "    result = agent.download_images(pexels_url, download_folder)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DOWNLOAD RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Status: {result['status']}\")\n",
    "    print(f\"Images found: {result['images_count']}\")\n",
    "    print(f\"Download path: {result['download_path']}\")\n",
    "    \n",
    "    if result['errors']:\n",
    "        print(f\"Errors: {result['errors']}\")\n",
    "    \n",
    "    print(\"\\nAgent Messages:\")\n",
    "    for message in result['messages']:\n",
    "        print(f\"- {message}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
